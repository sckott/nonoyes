---
title: "nonoyes - text analysis of Reply All podcast transcripts"
author: "Scott Chamberlain"
date: "August 25, 2016"
output: 
  md_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  cache.path = 'cache/'
)
```

[Reply All](https://gimletmedia.com/show/reply-all/) is a great podcast. I've been wanting to learn some text analysis tools, and transcripts from the podcast are on their site. 

Took some approaches outlined in the [tidytext](https://cran.rstudio.com/web/packages/tidytext/) package in [this vignette](https://cran.rstudio.com/web/packages/tidytext/vignettes/tidytext.html), and used the [tokenizers](https://cran.rstudio.com/web/packages/tokenizers) package, and some of the tidyverse.

## Setup

Load deps

```{r}
library("httr")
library("xml2")
library("stringi")
library("dplyr")
library("ggplot2")
library("tokenizers")
library("tidytext")
library("tidyr")
```

source helper functions

```{r}
source("funs.R")
```

set base url

```{r}
ra_base <- "https://gimletmedia.com/show/reply-all/episodes"
```

## URLs

Make all urls for each page of episodes

```{r}
urls <- c(ra_base, file.path(ra_base, "page", 2:8))
```

Get urls for each episode

```{r cache=TRUE}
res <- lapply(urls, get_urls)
```

Remove those that are rebroadcasts, updates, or revisited

```{r}
res <- grep("rebroadcast|update|revisited", unlist(res), value = TRUE, invert = TRUE)
```

## Episode names

Give some episodes numbers that don't have them

```{r}
epnames <- sub("/$", "", sub("https://gimletmedia.com/episode/", "", res))
epnames <- sub("the-anxiety-box", "8-the-anxiety-box", epnames)
epnames <- sub("french-connection", "10-french-connection", epnames)
epnames <- sub("ive-killed-people-and-i-have-hostages", "15-ive-killed-people-and-i-have-hostages", epnames)
epnames <- sub("6-this-proves-everything", "75-this-proves-everything", epnames)
epnames <- sub("zardulu", "56-zardulu", epnames)
```

## Transcripts

Get transcripts

```{r cache=TRUE}
txts <- lapply(res, transcript_fetch, sleep = 1)
```

Parse transcripts

```{r}
txtsp <- lapply(txts, transcript_parse)
```

## Summary word usage

Summarise data for each transcript

```{r}
dat <- stats::setNames(lapply(txtsp, function(m) {
  bind_rows(lapply(m, function(v) {
    tmp <- unname(vapply(v, nchar, 1))
    data_frame(
      n = length(tmp),
      mean = mean(tmp),
      n_laugh = count_word(v, "laugh"),
      n_groan = count_word(v, "groan")
    )
  }), .id = "name")
}), epnames)
```

Bind data together to single dataframe, and filter, summarise

```{r}
data <- bind_rows(dat, .id = "episode") %>%
  filter(!is.na(episode)) %>%
  filter(grepl("^PJ$|^ALEX GOLDMAN$", name)) %>%
  mutate(ep_no = as.numeric(strextract(episode, "^[0-9]+"))) %>%
  group_by(ep_no) %>%
  mutate(nrow = NROW(ep_no)) %>%
  ungroup() %>%
  filter(nrow == 2)
data
```

Number of words - seems PJ talks more, but didn't do quantiative comparison

```{r}
ggplot(data, aes(ep_no, n, colour = name)) +
  geom_point(size = 3, alpha = 0.5) +
  geom_line(aes(group = ep_no), colour = "black") +
  scale_color_discrete(labels = c('Alex', 'PJ'))
```

Laughs per episode - take home: PJ laughs a lot

```{r}
ggplot(data, aes(ep_no, n_laugh, colour = name)) +
  geom_point(size = 3, alpha = 0.5) +
  geom_line(aes(group = ep_no), colour = "black") +
  scale_color_discrete(labels = c('Alex', 'PJ'))
```

## Sentiment

```{r}
zero <- which(vapply(txtsp, length, 1) == 0)
txtsp_ <- Filter(function(x) length(x) != 0, txtsp)
```

Tokenize words, and create data_frame

```{r}
wordz <- stats::setNames(
  lapply(txtsp_, function(z) {
    bind_rows(
      if (is.null(try_tokenize(z$`ALEX GOLDMAN`))) {
        data_frame()
      } else {
        data_frame(
          name = "Alex",
          word = try_tokenize(z$`ALEX GOLDMAN`)
        )
      },
      if (is.null(try_tokenize(z$PJ))) {
        data_frame()
      } else {
        data_frame(
          name = "PJ",
          word = try_tokenize(z$PJ)
        )
      }
    )
  }), epnames[-zero])
```

Combine to single data_frame

```{r}
(wordz_df <- bind_rows(wordz, .id = "episode"))
```

Calculate sentiment using `tidytext`

```{r}
bing <- sentiments %>%
  filter(lexicon == "bing") %>%
  select(-score)
sent <- wordz_df %>%
  inner_join(bing) %>%
  count(name, episode, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ungroup() %>%
  filter(!is.na(episode)) %>%
  complete(episode, name) %>%
  mutate(ep_no = as.numeric(strextract(episode, "^[0-9]+")))
sent
```

Names separate

```{r}
ggplot(sent, aes(ep_no, sentiment, fill = name)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, ncol = 2, scales = "free_x")
```

Compare for each episode

```{r}
ggplot(sent, aes(ep_no, sentiment, fill = name)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.5), width = 0.6)
```

### Most common positive and negative words

Clearly, the word `like` is surely rarely used as a positive word meaning e.g., that they _like something_, but rather as the colloquial `like, totally` usage. So it's removed.

Alex

```{r}
sent_cont_plot(wordz_df, "Alex")
```

PJ

```{r}
sent_cont_plot(wordz_df, "PJ")
```
